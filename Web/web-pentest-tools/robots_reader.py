import requests
import re

domain = str(input("Enter Your Domain Like This Format * http(s)://www.example.com * >> "))
robots_path = domain + "/robots.txt"


try: 
    robots_page = requests.get(robots_path, "html.parser").text
    disallowed_paths = re.findall("Disallow\: \S{1,}", robots_page)

    for path in disallowed_paths:
        link = "[+] "+ domain+ path[10:]
        print(link)
except:
    print("EXIT........")
    


